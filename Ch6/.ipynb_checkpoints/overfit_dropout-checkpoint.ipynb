{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\김보경\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\김보경\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\김보경\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2982760532652393\n",
      "=== epoch:1, train acc:0.07333333333333333, test acc:0.0877 ===\n",
      "train loss:2.292403185797353\n",
      "train loss:2.2891173957422115\n",
      "train loss:2.299912227462252\n",
      "=== epoch:2, train acc:0.07, test acc:0.0906 ===\n",
      "train loss:2.296168681170754\n",
      "train loss:2.301941526600673\n",
      "train loss:2.299536748587386\n",
      "=== epoch:3, train acc:0.07333333333333333, test acc:0.0912 ===\n",
      "train loss:2.2912328581574273\n",
      "train loss:2.3018519613894375\n",
      "train loss:2.304701159585668\n",
      "=== epoch:4, train acc:0.07, test acc:0.0909 ===\n",
      "train loss:2.297444376044161\n",
      "train loss:2.3206394889320148\n",
      "train loss:2.2816213616469265\n",
      "=== epoch:5, train acc:0.08333333333333333, test acc:0.0926 ===\n",
      "train loss:2.2906929711824597\n",
      "train loss:2.2947120335555025\n",
      "train loss:2.263158230903355\n",
      "=== epoch:6, train acc:0.08666666666666667, test acc:0.0924 ===\n",
      "train loss:2.27819189471922\n",
      "train loss:2.2924382074238423\n",
      "train loss:2.283314785590452\n",
      "=== epoch:7, train acc:0.08666666666666667, test acc:0.0931 ===\n",
      "train loss:2.297389925019541\n",
      "train loss:2.28295385967289\n",
      "train loss:2.300809486132516\n",
      "=== epoch:8, train acc:0.07666666666666666, test acc:0.0933 ===\n",
      "train loss:2.299530840563084\n",
      "train loss:2.2762400788840313\n",
      "train loss:2.269252912835847\n",
      "=== epoch:9, train acc:0.09, test acc:0.0961 ===\n",
      "train loss:2.282664909715755\n",
      "train loss:2.281968364696518\n",
      "train loss:2.2778274496603528\n",
      "=== epoch:10, train acc:0.08333333333333333, test acc:0.0969 ===\n",
      "train loss:2.27697741240395\n",
      "train loss:2.257622584251962\n",
      "train loss:2.2889045766224254\n",
      "=== epoch:11, train acc:0.09, test acc:0.097 ===\n",
      "train loss:2.2701334512560383\n",
      "train loss:2.299827176490356\n",
      "train loss:2.2863741293938245\n",
      "=== epoch:12, train acc:0.08666666666666667, test acc:0.1003 ===\n",
      "train loss:2.2782554101001593\n",
      "train loss:2.291428097785878\n",
      "train loss:2.2611184894929917\n",
      "=== epoch:13, train acc:0.1, test acc:0.1034 ===\n",
      "train loss:2.2883821261753154\n",
      "train loss:2.289319483830177\n",
      "train loss:2.277762220441071\n",
      "=== epoch:14, train acc:0.11666666666666667, test acc:0.1097 ===\n",
      "train loss:2.270082106331617\n",
      "train loss:2.2871150691784705\n",
      "train loss:2.2751757270706436\n",
      "=== epoch:15, train acc:0.11666666666666667, test acc:0.1107 ===\n",
      "train loss:2.274960094812839\n",
      "train loss:2.2889556394180968\n",
      "train loss:2.3131923832806582\n",
      "=== epoch:16, train acc:0.12666666666666668, test acc:0.1199 ===\n",
      "train loss:2.2735196826647375\n",
      "train loss:2.2795372547668094\n",
      "train loss:2.2767610402373384\n",
      "=== epoch:17, train acc:0.13666666666666666, test acc:0.1247 ===\n",
      "train loss:2.2799478765894685\n",
      "train loss:2.284696726717409\n",
      "train loss:2.278733774675467\n",
      "=== epoch:18, train acc:0.13666666666666666, test acc:0.1294 ===\n",
      "train loss:2.2883600443040217\n",
      "train loss:2.27656586064568\n",
      "train loss:2.283461690820468\n",
      "=== epoch:19, train acc:0.14, test acc:0.1369 ===\n",
      "train loss:2.2684156777377105\n",
      "train loss:2.2832814530569703\n",
      "train loss:2.2776483554161495\n",
      "=== epoch:20, train acc:0.15, test acc:0.1459 ===\n",
      "train loss:2.2808309245871707\n",
      "train loss:2.274811078591974\n",
      "train loss:2.2960693820417997\n",
      "=== epoch:21, train acc:0.15, test acc:0.1527 ===\n",
      "train loss:2.267045943641642\n",
      "train loss:2.2809055472868467\n",
      "train loss:2.275213248390678\n",
      "=== epoch:22, train acc:0.15666666666666668, test acc:0.1582 ===\n",
      "train loss:2.2705490953880325\n",
      "train loss:2.2704529011076158\n",
      "train loss:2.274980859735609\n",
      "=== epoch:23, train acc:0.16, test acc:0.1596 ===\n",
      "train loss:2.274483281504571\n",
      "train loss:2.2807910315011983\n",
      "train loss:2.276533098743789\n",
      "=== epoch:24, train acc:0.17, test acc:0.1643 ===\n",
      "train loss:2.272586493052257\n",
      "train loss:2.2725886660132395\n",
      "train loss:2.2854327478990353\n",
      "=== epoch:25, train acc:0.16666666666666666, test acc:0.1689 ===\n",
      "train loss:2.2668087948908586\n",
      "train loss:2.250360526840866\n",
      "train loss:2.278529325018103\n",
      "=== epoch:26, train acc:0.18, test acc:0.1747 ===\n",
      "train loss:2.2768571296201863\n",
      "train loss:2.2765970541113916\n",
      "train loss:2.267881836444456\n",
      "=== epoch:27, train acc:0.18, test acc:0.175 ===\n",
      "train loss:2.275491729504773\n",
      "train loss:2.267903369190529\n",
      "train loss:2.258493556290738\n",
      "=== epoch:28, train acc:0.18, test acc:0.1778 ===\n",
      "train loss:2.2512378150673498\n",
      "train loss:2.259409150931927\n",
      "train loss:2.2626288795490956\n",
      "=== epoch:29, train acc:0.18666666666666668, test acc:0.1806 ===\n",
      "train loss:2.2543457557598545\n",
      "train loss:2.2573483549827595\n",
      "train loss:2.2637110119023562\n",
      "=== epoch:30, train acc:0.2, test acc:0.1843 ===\n",
      "train loss:2.2655659470193843\n",
      "train loss:2.2594156549064266\n",
      "train loss:2.261965593899501\n",
      "=== epoch:31, train acc:0.20666666666666667, test acc:0.1854 ===\n",
      "train loss:2.2609558114729498\n",
      "train loss:2.2693511339575663\n",
      "train loss:2.2584791501909574\n",
      "=== epoch:32, train acc:0.21666666666666667, test acc:0.1896 ===\n",
      "train loss:2.2740642762780445\n",
      "train loss:2.2544349279329405\n",
      "train loss:2.263853142834859\n",
      "=== epoch:33, train acc:0.21333333333333335, test acc:0.192 ===\n",
      "train loss:2.257744837228135\n",
      "train loss:2.2581399110054674\n",
      "train loss:2.2576946818390407\n",
      "=== epoch:34, train acc:0.21666666666666667, test acc:0.1927 ===\n",
      "train loss:2.2433992921849066\n",
      "train loss:2.2446169860244587\n",
      "train loss:2.2632418366456077\n",
      "=== epoch:35, train acc:0.22333333333333333, test acc:0.193 ===\n",
      "train loss:2.2629901525707554\n",
      "train loss:2.2631362927725944\n",
      "train loss:2.2694358536084542\n",
      "=== epoch:36, train acc:0.22333333333333333, test acc:0.1966 ===\n",
      "train loss:2.265513295577016\n",
      "train loss:2.2573907056497107\n",
      "train loss:2.2688774646965255\n",
      "=== epoch:37, train acc:0.22666666666666666, test acc:0.1979 ===\n",
      "train loss:2.277650879087003\n",
      "train loss:2.2451431978149423\n",
      "train loss:2.2445239172380775\n",
      "=== epoch:38, train acc:0.22666666666666666, test acc:0.2001 ===\n",
      "train loss:2.2439578430612155\n",
      "train loss:2.259043129201218\n",
      "train loss:2.258281503222247\n",
      "=== epoch:39, train acc:0.23333333333333334, test acc:0.2008 ===\n",
      "train loss:2.2596656790467593\n",
      "train loss:2.259871035219752\n",
      "train loss:2.251778058039296\n",
      "=== epoch:40, train acc:0.24, test acc:0.2016 ===\n",
      "train loss:2.2522173839694792\n",
      "train loss:2.2530279454765547\n",
      "train loss:2.2526454353548364\n",
      "=== epoch:41, train acc:0.24, test acc:0.2027 ===\n",
      "train loss:2.2597811468457065\n",
      "train loss:2.234696385114941\n",
      "train loss:2.255402456019207\n",
      "=== epoch:42, train acc:0.25, test acc:0.2025 ===\n",
      "train loss:2.244812793165954\n",
      "train loss:2.2396679698778703\n",
      "train loss:2.2450565052127804\n",
      "=== epoch:43, train acc:0.24666666666666667, test acc:0.203 ===\n",
      "train loss:2.2413340708480387\n",
      "train loss:2.2466397257092954\n",
      "train loss:2.2558544881366744\n",
      "=== epoch:44, train acc:0.25, test acc:0.2035 ===\n",
      "train loss:2.238408410077587\n",
      "train loss:2.2645682844844552\n",
      "train loss:2.252975627394377\n",
      "=== epoch:45, train acc:0.24333333333333335, test acc:0.2051 ===\n",
      "train loss:2.2511660961358264\n",
      "train loss:2.256897862865163\n",
      "train loss:2.2452560171366915\n",
      "=== epoch:46, train acc:0.24333333333333335, test acc:0.2058 ===\n",
      "train loss:2.259451546763427\n",
      "train loss:2.2489627920454485\n",
      "train loss:2.2676304859835175\n",
      "=== epoch:47, train acc:0.24333333333333335, test acc:0.2057 ===\n",
      "train loss:2.2197349439004097\n",
      "train loss:2.2369761638461303\n",
      "train loss:2.260948106857248\n",
      "=== epoch:48, train acc:0.24333333333333335, test acc:0.2069 ===\n",
      "train loss:2.22651926838458\n",
      "train loss:2.2338712338276854\n",
      "train loss:2.24390599412843\n",
      "=== epoch:49, train acc:0.24333333333333335, test acc:0.2073 ===\n",
      "train loss:2.22690694344767\n",
      "train loss:2.2541789330929305\n",
      "train loss:2.2488400223342837\n",
      "=== epoch:50, train acc:0.24333333333333335, test acc:0.2071 ===\n",
      "train loss:2.2475708439491235\n",
      "train loss:2.2150221705616095\n",
      "train loss:2.2545421033596678\n",
      "=== epoch:51, train acc:0.24333333333333335, test acc:0.2077 ===\n",
      "train loss:2.229473572005806\n",
      "train loss:2.24984702768318\n",
      "train loss:2.2448452957185996\n",
      "=== epoch:52, train acc:0.24333333333333335, test acc:0.2078 ===\n",
      "train loss:2.235036087628495\n",
      "train loss:2.239643805068838\n",
      "train loss:2.227420936761831\n",
      "=== epoch:53, train acc:0.25, test acc:0.2082 ===\n",
      "train loss:2.2454492187268484\n",
      "train loss:2.2502839929353082\n",
      "train loss:2.218228200287678\n",
      "=== epoch:54, train acc:0.25, test acc:0.2093 ===\n",
      "train loss:2.2454164736405566\n",
      "train loss:2.2286799549762137\n",
      "train loss:2.2381457286599105\n",
      "=== epoch:55, train acc:0.25, test acc:0.2089 ===\n",
      "train loss:2.238475639574671\n",
      "train loss:2.2382654742073695\n",
      "train loss:2.223870782110418\n",
      "=== epoch:56, train acc:0.25, test acc:0.2086 ===\n",
      "train loss:2.2330982996036517\n",
      "train loss:2.264912800424117\n",
      "train loss:2.2295492454293515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.24666666666666667, test acc:0.2091 ===\n",
      "train loss:2.229303429835944\n",
      "train loss:2.2235036219494377\n",
      "train loss:2.2200281862568025\n",
      "=== epoch:58, train acc:0.24666666666666667, test acc:0.2105 ===\n",
      "train loss:2.2287072421317804\n",
      "train loss:2.2197186391994204\n",
      "train loss:2.2373858736506937\n",
      "=== epoch:59, train acc:0.24666666666666667, test acc:0.2111 ===\n",
      "train loss:2.2376531102361032\n",
      "train loss:2.2373343716200615\n",
      "train loss:2.236463527495236\n",
      "=== epoch:60, train acc:0.24666666666666667, test acc:0.2116 ===\n",
      "train loss:2.2338738439069004\n",
      "train loss:2.2384579264621776\n",
      "train loss:2.2178058318470604\n",
      "=== epoch:61, train acc:0.24666666666666667, test acc:0.2113 ===\n",
      "train loss:2.207880227869734\n",
      "train loss:2.2274365128835907\n",
      "train loss:2.2378174550851786\n",
      "=== epoch:62, train acc:0.25, test acc:0.2109 ===\n",
      "train loss:2.2372314921635192\n",
      "train loss:2.2288164377764814\n",
      "train loss:2.2383471405390587\n",
      "=== epoch:63, train acc:0.25, test acc:0.2101 ===\n",
      "train loss:2.220756217548837\n",
      "train loss:2.212325674373628\n",
      "train loss:2.2322416575452992\n",
      "=== epoch:64, train acc:0.25333333333333335, test acc:0.2106 ===\n",
      "train loss:2.221826175140813\n",
      "train loss:2.2164030332242595\n",
      "train loss:2.2122724664340696\n",
      "=== epoch:65, train acc:0.25, test acc:0.2105 ===\n",
      "train loss:2.214308713617565\n",
      "train loss:2.226832311465361\n",
      "train loss:2.236943515903222\n",
      "=== epoch:66, train acc:0.25, test acc:0.2108 ===\n",
      "train loss:2.2283110234445904\n",
      "train loss:2.219356995731188\n",
      "train loss:2.226223019373662\n",
      "=== epoch:67, train acc:0.25, test acc:0.2115 ===\n",
      "train loss:2.2293799849518403\n",
      "train loss:2.241860509027827\n",
      "train loss:2.2156031481559215\n",
      "=== epoch:68, train acc:0.25, test acc:0.212 ===\n",
      "train loss:2.2004767937945955\n",
      "train loss:2.204515801268445\n",
      "train loss:2.239899759667167\n",
      "=== epoch:69, train acc:0.25, test acc:0.2118 ===\n",
      "train loss:2.231436795970318\n",
      "train loss:2.2544958262163983\n",
      "train loss:2.2146812264600864\n",
      "=== epoch:70, train acc:0.25, test acc:0.2122 ===\n",
      "train loss:2.2289809620225203\n",
      "train loss:2.2138707294873443\n",
      "train loss:2.1982910038249215\n",
      "=== epoch:71, train acc:0.25, test acc:0.2122 ===\n",
      "train loss:2.2204342927613405\n",
      "train loss:2.1996345580412857\n",
      "train loss:2.224923697133643\n",
      "=== epoch:72, train acc:0.25, test acc:0.2126 ===\n",
      "train loss:2.2047124155263\n",
      "train loss:2.2187451376022924\n",
      "train loss:2.220997972756551\n",
      "=== epoch:73, train acc:0.25333333333333335, test acc:0.2134 ===\n",
      "train loss:2.2030022014020103\n",
      "train loss:2.2238644471414677\n",
      "train loss:2.229941386638282\n",
      "=== epoch:74, train acc:0.25333333333333335, test acc:0.2131 ===\n",
      "train loss:2.185682938600792\n",
      "train loss:2.2032938917662417\n",
      "train loss:2.2028563428622787\n",
      "=== epoch:75, train acc:0.25333333333333335, test acc:0.2131 ===\n",
      "train loss:2.2147360256716064\n",
      "train loss:2.211150761651807\n",
      "train loss:2.1939970917100178\n",
      "=== epoch:76, train acc:0.25, test acc:0.2129 ===\n",
      "train loss:2.1977178210793333\n",
      "train loss:2.246734694794422\n",
      "train loss:2.2285134719452238\n",
      "=== epoch:77, train acc:0.25333333333333335, test acc:0.2134 ===\n",
      "train loss:2.212151871082517\n",
      "train loss:2.227683022999613\n",
      "train loss:2.1862110495227505\n",
      "=== epoch:78, train acc:0.25333333333333335, test acc:0.2139 ===\n",
      "train loss:2.230322690330251\n",
      "train loss:2.190249155633287\n",
      "train loss:2.227925061334083\n",
      "=== epoch:79, train acc:0.25666666666666665, test acc:0.2143 ===\n",
      "train loss:2.1932073150249276\n",
      "train loss:2.1644185863969514\n",
      "train loss:2.216793829351071\n",
      "=== epoch:80, train acc:0.25666666666666665, test acc:0.2138 ===\n",
      "train loss:2.2309500775944127\n",
      "train loss:2.2117499872518915\n",
      "train loss:2.235735555376134\n",
      "=== epoch:81, train acc:0.25666666666666665, test acc:0.2145 ===\n",
      "train loss:2.2066749313644616\n",
      "train loss:2.219638902311855\n",
      "train loss:2.2016567991069618\n",
      "=== epoch:82, train acc:0.26, test acc:0.2157 ===\n",
      "train loss:2.2151617633376888\n",
      "train loss:2.2034879132762186\n",
      "train loss:2.208037860367741\n",
      "=== epoch:83, train acc:0.25666666666666665, test acc:0.2147 ===\n",
      "train loss:2.204206146334172\n",
      "train loss:2.1996379010515033\n",
      "train loss:2.183836488259948\n",
      "=== epoch:84, train acc:0.25666666666666665, test acc:0.2149 ===\n",
      "train loss:2.181278985965333\n",
      "train loss:2.217497651632411\n",
      "train loss:2.1803295509111047\n",
      "=== epoch:85, train acc:0.2633333333333333, test acc:0.2152 ===\n",
      "train loss:2.1814056349104725\n",
      "train loss:2.1816118668686872\n",
      "train loss:2.1854393025261976\n",
      "=== epoch:86, train acc:0.26, test acc:0.2154 ===\n",
      "train loss:2.1844292250650046\n",
      "train loss:2.2044185655928015\n",
      "train loss:2.1940961692478513\n",
      "=== epoch:87, train acc:0.26, test acc:0.2152 ===\n",
      "train loss:2.1499440395520764\n",
      "train loss:2.206876029062448\n",
      "train loss:2.1793283119214455\n",
      "=== epoch:88, train acc:0.2633333333333333, test acc:0.2153 ===\n",
      "train loss:2.199287811258354\n",
      "train loss:2.182719248380503\n",
      "train loss:2.2056500222828617\n",
      "=== epoch:89, train acc:0.26, test acc:0.2157 ===\n",
      "train loss:2.1844628473725014\n",
      "train loss:2.1711456045853277\n",
      "train loss:2.1946027245291737\n",
      "=== epoch:90, train acc:0.26, test acc:0.2157 ===\n",
      "train loss:2.1746161252688716\n",
      "train loss:2.2069201690560574\n",
      "train loss:2.1646985634337663\n",
      "=== epoch:91, train acc:0.26, test acc:0.215 ===\n",
      "train loss:2.1979663530823923\n",
      "train loss:2.1722574869468105\n",
      "train loss:2.1425006425042454\n",
      "=== epoch:92, train acc:0.25666666666666665, test acc:0.2148 ===\n",
      "train loss:2.175268665338265\n",
      "train loss:2.2053426089664567\n",
      "train loss:2.1688523380362357\n",
      "=== epoch:93, train acc:0.25666666666666665, test acc:0.2152 ===\n",
      "train loss:2.169325747831317\n",
      "train loss:2.1575808862888652\n",
      "train loss:2.167516883101727\n",
      "=== epoch:94, train acc:0.25666666666666665, test acc:0.2149 ===\n",
      "train loss:2.1859979767986295\n",
      "train loss:2.1555875475406063\n",
      "train loss:2.1920477867539163\n",
      "=== epoch:95, train acc:0.25666666666666665, test acc:0.2153 ===\n",
      "train loss:2.1392562277567833\n",
      "train loss:2.1690520766875085\n",
      "train loss:2.1760754683584116\n",
      "=== epoch:96, train acc:0.25666666666666665, test acc:0.2148 ===\n",
      "train loss:2.180076724416316\n",
      "train loss:2.1976976338975063\n",
      "train loss:2.178443023136723\n",
      "=== epoch:97, train acc:0.25666666666666665, test acc:0.2152 ===\n",
      "train loss:2.16951602961358\n",
      "train loss:2.1771394817809844\n",
      "train loss:2.171352787650073\n",
      "=== epoch:98, train acc:0.25333333333333335, test acc:0.215 ===\n",
      "train loss:2.1541029549924797\n",
      "train loss:2.17913494140529\n",
      "train loss:2.1833211705037328\n",
      "=== epoch:99, train acc:0.25333333333333335, test acc:0.2155 ===\n",
      "train loss:2.1504110612335934\n",
      "train loss:2.122282772938117\n",
      "train loss:2.162490040834498\n",
      "=== epoch:100, train acc:0.25333333333333335, test acc:0.2154 ===\n",
      "train loss:2.189641526935252\n",
      "train loss:2.1940880086764527\n",
      "train loss:2.1834742249775165\n",
      "=== epoch:101, train acc:0.25333333333333335, test acc:0.2156 ===\n",
      "train loss:2.1886475621781227\n",
      "train loss:2.1983512702013983\n",
      "train loss:2.1566947719909417\n",
      "=== epoch:102, train acc:0.25333333333333335, test acc:0.2156 ===\n",
      "train loss:2.168127524574743\n",
      "train loss:2.17513691528718\n",
      "train loss:2.1396795915217672\n",
      "=== epoch:103, train acc:0.25333333333333335, test acc:0.2153 ===\n",
      "train loss:2.1574574029269398\n",
      "train loss:2.169719277299117\n",
      "train loss:2.169922435462446\n",
      "=== epoch:104, train acc:0.25666666666666665, test acc:0.2156 ===\n",
      "train loss:2.1780377358469836\n",
      "train loss:2.194681390860876\n",
      "train loss:2.164623453280608\n",
      "=== epoch:105, train acc:0.26, test acc:0.216 ===\n",
      "train loss:2.146417624954401\n",
      "train loss:2.193297022577089\n",
      "train loss:2.166111399336173\n",
      "=== epoch:106, train acc:0.25666666666666665, test acc:0.2164 ===\n",
      "train loss:2.187471648823875\n",
      "train loss:2.1579801499051383\n",
      "train loss:2.1987474418772988\n",
      "=== epoch:107, train acc:0.26, test acc:0.2177 ===\n",
      "train loss:2.1524343802527914\n",
      "train loss:2.156883859147027\n",
      "train loss:2.119723393795723\n",
      "=== epoch:108, train acc:0.25666666666666665, test acc:0.2172 ===\n",
      "train loss:2.1538014686142235\n",
      "train loss:2.1387187145453437\n",
      "train loss:2.1571332060343376\n",
      "=== epoch:109, train acc:0.25666666666666665, test acc:0.2166 ===\n",
      "train loss:2.1648058517162094\n",
      "train loss:2.1471667434401835\n",
      "train loss:2.13098703666815\n",
      "=== epoch:110, train acc:0.25333333333333335, test acc:0.2163 ===\n",
      "train loss:2.1550992050633115\n",
      "train loss:2.10937947447928\n",
      "train loss:2.150002839441552\n",
      "=== epoch:111, train acc:0.25666666666666665, test acc:0.2168 ===\n",
      "train loss:2.1517173788939643\n",
      "train loss:2.130189078492963\n",
      "train loss:2.1725149022674035\n",
      "=== epoch:112, train acc:0.25333333333333335, test acc:0.2164 ===\n",
      "train loss:2.1477629575267057\n",
      "train loss:2.16897179729809\n",
      "train loss:2.159254326431465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:113, train acc:0.25666666666666665, test acc:0.2168 ===\n",
      "train loss:2.1462672526952513\n",
      "train loss:2.170518154101738\n",
      "train loss:2.122653763222228\n",
      "=== epoch:114, train acc:0.25666666666666665, test acc:0.2171 ===\n",
      "train loss:2.134579181426161\n",
      "train loss:2.1266630948135337\n",
      "train loss:2.1427034508471814\n",
      "=== epoch:115, train acc:0.25666666666666665, test acc:0.2168 ===\n",
      "train loss:2.139415005666809\n",
      "train loss:2.11981757659537\n",
      "train loss:2.1174684140022753\n",
      "=== epoch:116, train acc:0.25, test acc:0.2164 ===\n",
      "train loss:2.1061588603541224\n",
      "train loss:2.127238200081204\n",
      "train loss:2.1459619588444965\n",
      "=== epoch:117, train acc:0.25, test acc:0.2166 ===\n",
      "train loss:2.089115963938654\n",
      "train loss:2.1253753370616306\n",
      "train loss:2.1040643508260604\n",
      "=== epoch:118, train acc:0.25333333333333335, test acc:0.2152 ===\n",
      "train loss:2.1600185350841836\n",
      "train loss:2.131855011695716\n",
      "train loss:2.126576126400741\n",
      "=== epoch:119, train acc:0.25333333333333335, test acc:0.2158 ===\n",
      "train loss:2.1093125736374296\n",
      "train loss:2.0803269064434966\n",
      "train loss:2.0775723758403317\n",
      "=== epoch:120, train acc:0.25, test acc:0.2138 ===\n",
      "train loss:2.0813399744861028\n",
      "train loss:2.123535852684097\n",
      "train loss:2.1188938650198867\n",
      "=== epoch:121, train acc:0.25, test acc:0.2133 ===\n",
      "train loss:2.12887499224101\n",
      "train loss:2.1398472030488347\n",
      "train loss:2.075310260997905\n",
      "=== epoch:122, train acc:0.24666666666666667, test acc:0.2127 ===\n",
      "train loss:2.131571032250687\n",
      "train loss:2.1214774365345757\n",
      "train loss:2.154613520411949\n",
      "=== epoch:123, train acc:0.25, test acc:0.2123 ===\n",
      "train loss:2.1031183974212913\n",
      "train loss:2.0526429760338742\n",
      "train loss:2.1266217722521477\n",
      "=== epoch:124, train acc:0.24666666666666667, test acc:0.2123 ===\n",
      "train loss:2.1269310814444236\n",
      "train loss:2.132591464818723\n",
      "train loss:2.138589213458335\n",
      "=== epoch:125, train acc:0.25, test acc:0.213 ===\n",
      "train loss:2.1184164780962504\n",
      "train loss:2.100928990122951\n",
      "train loss:2.1264261146780683\n",
      "=== epoch:126, train acc:0.25, test acc:0.2137 ===\n",
      "train loss:2.1309616217627534\n",
      "train loss:2.0934473818557198\n",
      "train loss:2.101316488480505\n",
      "=== epoch:127, train acc:0.24666666666666667, test acc:0.214 ===\n",
      "train loss:2.0980916420921627\n",
      "train loss:2.1501854014896002\n",
      "train loss:2.0977117593547616\n",
      "=== epoch:128, train acc:0.24666666666666667, test acc:0.2145 ===\n",
      "train loss:2.116436254109436\n",
      "train loss:2.1369414050096127\n",
      "train loss:2.1170589905754573\n",
      "=== epoch:129, train acc:0.24666666666666667, test acc:0.2152 ===\n",
      "train loss:2.0473570577360904\n",
      "train loss:2.070022930768943\n",
      "train loss:2.055703857932518\n",
      "=== epoch:130, train acc:0.24666666666666667, test acc:0.2139 ===\n",
      "train loss:2.095669440917589\n",
      "train loss:2.1386023598599793\n",
      "train loss:2.065458189159059\n",
      "=== epoch:131, train acc:0.25, test acc:0.2143 ===\n",
      "train loss:2.094170757148254\n",
      "train loss:2.1434236413232552\n",
      "train loss:2.1067827162360038\n",
      "=== epoch:132, train acc:0.25, test acc:0.2142 ===\n",
      "train loss:2.0801400751042407\n",
      "train loss:2.078740627039798\n",
      "train loss:2.069732789960091\n",
      "=== epoch:133, train acc:0.25, test acc:0.214 ===\n",
      "train loss:2.112180538942602\n",
      "train loss:2.1398185708956006\n",
      "train loss:2.1068105455202777\n",
      "=== epoch:134, train acc:0.25333333333333335, test acc:0.2146 ===\n",
      "train loss:2.1334878677636757\n",
      "train loss:2.0594097397000932\n",
      "train loss:2.112069576634468\n",
      "=== epoch:135, train acc:0.25333333333333335, test acc:0.2144 ===\n",
      "train loss:2.054812374760949\n",
      "train loss:2.085037464708003\n",
      "train loss:2.081347461818616\n",
      "=== epoch:136, train acc:0.25333333333333335, test acc:0.2147 ===\n",
      "train loss:2.124450579093598\n",
      "train loss:2.0666110737466905\n",
      "train loss:2.0916418921587567\n",
      "=== epoch:137, train acc:0.25333333333333335, test acc:0.2154 ===\n",
      "train loss:2.073656106495782\n",
      "train loss:2.0590334657725733\n",
      "train loss:2.0513927248097885\n",
      "=== epoch:138, train acc:0.25333333333333335, test acc:0.2157 ===\n",
      "train loss:2.003484633114086\n",
      "train loss:2.0633331107780086\n",
      "train loss:1.9964832984851264\n",
      "=== epoch:139, train acc:0.25333333333333335, test acc:0.2149 ===\n",
      "train loss:2.0759861926708165\n",
      "train loss:2.0511123239956772\n",
      "train loss:2.0941130812528157\n",
      "=== epoch:140, train acc:0.25, test acc:0.2148 ===\n",
      "train loss:2.038906818553926\n",
      "train loss:2.0756068864058017\n",
      "train loss:2.103493837046325\n",
      "=== epoch:141, train acc:0.25, test acc:0.2153 ===\n",
      "train loss:2.0400306548018334\n",
      "train loss:2.0609786923933693\n",
      "train loss:2.024633435982129\n",
      "=== epoch:142, train acc:0.25, test acc:0.2159 ===\n",
      "train loss:2.0683223247938995\n",
      "train loss:2.0197621081942425\n",
      "train loss:2.073120310162582\n",
      "=== epoch:143, train acc:0.25333333333333335, test acc:0.2144 ===\n",
      "train loss:2.029595596611418\n",
      "train loss:2.0513652940539\n",
      "train loss:2.044179332845748\n",
      "=== epoch:144, train acc:0.25, test acc:0.2145 ===\n",
      "train loss:2.036995010798858\n",
      "train loss:2.0867270574312036\n",
      "train loss:2.0525882943146736\n",
      "=== epoch:145, train acc:0.25, test acc:0.2152 ===\n",
      "train loss:2.0789221525926216\n",
      "train loss:2.090491367098335\n",
      "train loss:2.052856446598575\n",
      "=== epoch:146, train acc:0.25, test acc:0.2153 ===\n",
      "train loss:2.0577598102756736\n",
      "train loss:2.0481207660584206\n",
      "train loss:2.0289332399697573\n",
      "=== epoch:147, train acc:0.25, test acc:0.2152 ===\n",
      "train loss:2.0251346080825585\n",
      "train loss:2.076458907885832\n",
      "train loss:2.031365433812789\n",
      "=== epoch:148, train acc:0.25333333333333335, test acc:0.2155 ===\n",
      "train loss:1.997968334241787\n",
      "train loss:1.980986022263145\n",
      "train loss:2.0090405144363985\n",
      "=== epoch:149, train acc:0.25333333333333335, test acc:0.2159 ===\n",
      "train loss:2.0699758636607593\n",
      "train loss:2.027175382339796\n",
      "train loss:2.0482339547177\n",
      "=== epoch:150, train acc:0.25333333333333335, test acc:0.2156 ===\n",
      "train loss:2.013922464434408\n",
      "train loss:2.0461399628060515\n",
      "train loss:2.031532679862412\n",
      "=== epoch:151, train acc:0.25333333333333335, test acc:0.2154 ===\n",
      "train loss:1.9932062052006532\n",
      "train loss:2.0486774497943996\n",
      "train loss:2.0623656727283404\n",
      "=== epoch:152, train acc:0.25333333333333335, test acc:0.2155 ===\n",
      "train loss:2.043093624951037\n",
      "train loss:1.9660735388463024\n",
      "train loss:1.9553661775908495\n",
      "=== epoch:153, train acc:0.25, test acc:0.2149 ===\n",
      "train loss:2.0148250488716517\n",
      "train loss:2.013958991056889\n",
      "train loss:2.081384389704655\n",
      "=== epoch:154, train acc:0.25333333333333335, test acc:0.2157 ===\n",
      "train loss:2.0113114671230874\n",
      "train loss:2.0759078249847724\n",
      "train loss:2.0070863853345147\n",
      "=== epoch:155, train acc:0.25666666666666665, test acc:0.2181 ===\n",
      "train loss:1.9983921049481514\n",
      "train loss:2.0396681792760645\n",
      "train loss:1.9877721926264693\n",
      "=== epoch:156, train acc:0.25666666666666665, test acc:0.2192 ===\n",
      "train loss:1.9536172646919732\n",
      "train loss:2.008389335631875\n",
      "train loss:2.001055079891179\n",
      "=== epoch:157, train acc:0.25666666666666665, test acc:0.2181 ===\n",
      "train loss:2.0045180870089094\n",
      "train loss:2.046305937376993\n",
      "train loss:2.013587233480786\n",
      "=== epoch:158, train acc:0.25666666666666665, test acc:0.2185 ===\n",
      "train loss:1.9639375858483639\n",
      "train loss:1.948984745282646\n",
      "train loss:1.9754980518905172\n",
      "=== epoch:159, train acc:0.25, test acc:0.2168 ===\n",
      "train loss:2.0161908506390103\n",
      "train loss:1.963625135886732\n",
      "train loss:2.029454025733315\n",
      "=== epoch:160, train acc:0.25, test acc:0.2182 ===\n",
      "train loss:2.0585170092162293\n",
      "train loss:2.0123993476989073\n",
      "train loss:2.0717439355833847\n",
      "=== epoch:161, train acc:0.25333333333333335, test acc:0.2193 ===\n",
      "train loss:1.9585581595020363\n",
      "train loss:1.9197623925953275\n",
      "train loss:1.9684889812022235\n",
      "=== epoch:162, train acc:0.25333333333333335, test acc:0.2187 ===\n",
      "train loss:1.9552595429923587\n",
      "train loss:1.9277045256146892\n",
      "train loss:2.0146921743748334\n",
      "=== epoch:163, train acc:0.25, test acc:0.2181 ===\n",
      "train loss:1.999987861740095\n",
      "train loss:1.934674595284752\n",
      "train loss:1.984746639762327\n",
      "=== epoch:164, train acc:0.25333333333333335, test acc:0.2189 ===\n",
      "train loss:1.9924133462302547\n",
      "train loss:1.9188535420656871\n",
      "train loss:2.0123012416590655\n",
      "=== epoch:165, train acc:0.26, test acc:0.2198 ===\n",
      "train loss:2.0231214090956375\n",
      "train loss:1.956559950877566\n",
      "train loss:1.9177722209343522\n",
      "=== epoch:166, train acc:0.26666666666666666, test acc:0.221 ===\n",
      "train loss:1.9706077689478507\n",
      "train loss:1.999471411142745\n",
      "train loss:1.9786950068804228\n",
      "=== epoch:167, train acc:0.26666666666666666, test acc:0.221 ===\n",
      "train loss:1.9446418128170242\n",
      "train loss:1.9314953918577413\n",
      "train loss:1.9403893400547216\n",
      "=== epoch:168, train acc:0.26666666666666666, test acc:0.2204 ===\n",
      "train loss:1.946865749247663\n",
      "train loss:2.0280563877713367\n",
      "train loss:1.9409952925712264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:169, train acc:0.2633333333333333, test acc:0.2195 ===\n",
      "train loss:1.864786549592881\n",
      "train loss:1.9440210572851544\n",
      "train loss:1.9078621270005904\n",
      "=== epoch:170, train acc:0.25666666666666665, test acc:0.2191 ===\n",
      "train loss:1.9447374035183784\n",
      "train loss:1.9040463462158248\n",
      "train loss:1.9294170307617031\n",
      "=== epoch:171, train acc:0.26, test acc:0.2195 ===\n",
      "train loss:1.985000771041515\n",
      "train loss:1.9263351464585812\n",
      "train loss:1.9732615345904054\n",
      "=== epoch:172, train acc:0.26, test acc:0.2198 ===\n",
      "train loss:1.9872879338497509\n",
      "train loss:1.8975481348468106\n",
      "train loss:1.9765315084749135\n",
      "=== epoch:173, train acc:0.26666666666666666, test acc:0.2217 ===\n",
      "train loss:1.9385502435126518\n",
      "train loss:1.904438800673142\n",
      "train loss:1.912941846081653\n",
      "=== epoch:174, train acc:0.25666666666666665, test acc:0.2199 ===\n",
      "train loss:1.946218023962406\n",
      "train loss:1.9792106192651848\n",
      "train loss:2.0091804182546555\n",
      "=== epoch:175, train acc:0.27, test acc:0.2233 ===\n",
      "train loss:1.9105491754507036\n",
      "train loss:1.9410634459045761\n",
      "train loss:1.9566426759503068\n",
      "=== epoch:176, train acc:0.27, test acc:0.2239 ===\n",
      "train loss:1.9254644714784153\n",
      "train loss:1.955251861862107\n",
      "train loss:1.8989951599994617\n",
      "=== epoch:177, train acc:0.26666666666666666, test acc:0.2258 ===\n",
      "train loss:1.8913337530480974\n",
      "train loss:2.0231856393217758\n",
      "train loss:1.9591409130420334\n",
      "=== epoch:178, train acc:0.27, test acc:0.2278 ===\n",
      "train loss:1.8894370324504854\n",
      "train loss:1.8687961494736287\n",
      "train loss:1.9202896897274804\n",
      "=== epoch:179, train acc:0.2733333333333333, test acc:0.2294 ===\n",
      "train loss:1.9364992755685781\n",
      "train loss:1.9560176215244798\n",
      "train loss:1.9133000647741063\n",
      "=== epoch:180, train acc:0.27666666666666667, test acc:0.23 ===\n",
      "train loss:1.883794860258472\n",
      "train loss:1.9725583437728944\n",
      "train loss:1.916279792240367\n",
      "=== epoch:181, train acc:0.27666666666666667, test acc:0.2319 ===\n",
      "train loss:1.8887980941590252\n",
      "train loss:1.9746940892360678\n",
      "train loss:1.997626909898616\n",
      "=== epoch:182, train acc:0.27666666666666667, test acc:0.2331 ===\n",
      "train loss:1.944084997857515\n",
      "train loss:1.9050669028452114\n",
      "train loss:1.9941504965538246\n",
      "=== epoch:183, train acc:0.2733333333333333, test acc:0.2325 ===\n",
      "train loss:1.9007614948810758\n",
      "train loss:1.882015389630237\n",
      "train loss:1.8623971636901238\n",
      "=== epoch:184, train acc:0.27666666666666667, test acc:0.2327 ===\n",
      "train loss:1.8745682254349194\n",
      "train loss:1.9199580286443423\n",
      "train loss:1.8823682223252671\n",
      "=== epoch:185, train acc:0.27666666666666667, test acc:0.2328 ===\n",
      "train loss:1.943578220461648\n",
      "train loss:1.8538249642064222\n",
      "train loss:1.9313140980082577\n",
      "=== epoch:186, train acc:0.28, test acc:0.2372 ===\n",
      "train loss:1.90891802008908\n",
      "train loss:1.7713971783039846\n",
      "train loss:1.868524312619835\n",
      "=== epoch:187, train acc:0.28, test acc:0.236 ===\n",
      "train loss:1.8912355949184596\n",
      "train loss:1.9737076694057338\n",
      "train loss:1.8795122722494697\n",
      "=== epoch:188, train acc:0.29, test acc:0.2406 ===\n",
      "train loss:1.895340671642397\n",
      "train loss:1.8744088746936385\n",
      "train loss:1.9591354109713535\n",
      "=== epoch:189, train acc:0.3, test acc:0.2416 ===\n",
      "train loss:1.8598557326466068\n",
      "train loss:1.8988070649770055\n",
      "train loss:1.8692122643473945\n",
      "=== epoch:190, train acc:0.3, test acc:0.2418 ===\n",
      "train loss:1.848959137420954\n",
      "train loss:1.927468430839558\n",
      "train loss:1.8066214269029144\n",
      "=== epoch:191, train acc:0.3, test acc:0.2414 ===\n",
      "train loss:1.8131011958245051\n",
      "train loss:1.8173206053232527\n",
      "train loss:1.8196215132045586\n",
      "=== epoch:192, train acc:0.28, test acc:0.2363 ===\n",
      "train loss:1.8455668196943986\n",
      "train loss:1.8608996195962184\n",
      "train loss:1.869733749174893\n",
      "=== epoch:193, train acc:0.29333333333333333, test acc:0.2405 ===\n",
      "train loss:1.8299818941597261\n",
      "train loss:1.8897440139193251\n",
      "train loss:1.879511440991498\n",
      "=== epoch:194, train acc:0.2966666666666667, test acc:0.2425 ===\n",
      "train loss:1.807758991921837\n",
      "train loss:1.7872451717128197\n",
      "train loss:1.9163342422260035\n",
      "=== epoch:195, train acc:0.3, test acc:0.2433 ===\n",
      "train loss:1.8409266260808133\n",
      "train loss:1.8289749190228037\n",
      "train loss:1.8388447601832982\n",
      "=== epoch:196, train acc:0.3, test acc:0.2434 ===\n",
      "train loss:1.8830093166063901\n",
      "train loss:1.8822478827170084\n",
      "train loss:1.9029845259965157\n",
      "=== epoch:197, train acc:0.3, test acc:0.247 ===\n",
      "train loss:1.8817725584140956\n",
      "train loss:1.831212112832536\n",
      "train loss:1.9002679140254317\n",
      "=== epoch:198, train acc:0.2966666666666667, test acc:0.2478 ===\n",
      "train loss:1.8936338618995487\n",
      "train loss:1.8200503666367127\n",
      "train loss:1.8166721436237183\n",
      "=== epoch:199, train acc:0.3, test acc:0.2492 ===\n",
      "train loss:1.8324717577537657\n",
      "train loss:1.8159326724246698\n",
      "train loss:1.7950439399666491\n",
      "=== epoch:200, train acc:0.3, test acc:0.2463 ===\n",
      "train loss:1.8582664982875396\n",
      "train loss:1.7642310044659542\n",
      "train loss:1.8818340639381412\n",
      "=== epoch:201, train acc:0.2966666666666667, test acc:0.2471 ===\n",
      "train loss:1.8317972867283059\n",
      "train loss:1.7644855810825155\n",
      "train loss:1.7825597709502516\n",
      "=== epoch:202, train acc:0.30333333333333334, test acc:0.2475 ===\n",
      "train loss:1.8211598080250198\n",
      "train loss:1.8491296372350288\n",
      "train loss:1.8517564539255937\n",
      "=== epoch:203, train acc:0.31333333333333335, test acc:0.2526 ===\n",
      "train loss:1.870600705243005\n",
      "train loss:1.752719190786541\n",
      "train loss:1.707466601176015\n",
      "=== epoch:204, train acc:0.31333333333333335, test acc:0.2507 ===\n",
      "train loss:1.8953301789513473\n",
      "train loss:1.8280886627803696\n",
      "train loss:1.8588425049449269\n",
      "=== epoch:205, train acc:0.30666666666666664, test acc:0.2523 ===\n",
      "train loss:1.7762503128202352\n",
      "train loss:1.867271954119545\n",
      "train loss:1.78702415815056\n",
      "=== epoch:206, train acc:0.31333333333333335, test acc:0.2595 ===\n",
      "train loss:1.837822176292338\n",
      "train loss:1.7890041722675982\n",
      "train loss:1.7562003051255433\n",
      "=== epoch:207, train acc:0.31333333333333335, test acc:0.2598 ===\n",
      "train loss:1.8541434359792945\n",
      "train loss:1.8447296250109892\n",
      "train loss:1.8455444739646703\n",
      "=== epoch:208, train acc:0.32, test acc:0.2648 ===\n",
      "train loss:1.786818223503297\n",
      "train loss:1.826122677177728\n",
      "train loss:1.7611094071254467\n",
      "=== epoch:209, train acc:0.3233333333333333, test acc:0.2656 ===\n",
      "train loss:1.8306789286726055\n",
      "train loss:1.756102555563404\n",
      "train loss:1.8016567737423077\n",
      "=== epoch:210, train acc:0.31666666666666665, test acc:0.2662 ===\n",
      "train loss:1.7667717799110405\n",
      "train loss:1.836948728593213\n",
      "train loss:1.7792853592181939\n",
      "=== epoch:211, train acc:0.31666666666666665, test acc:0.2688 ===\n",
      "train loss:1.8296165375059166\n",
      "train loss:1.7612111731590472\n",
      "train loss:1.7503238626677997\n",
      "=== epoch:212, train acc:0.3233333333333333, test acc:0.2696 ===\n",
      "train loss:1.8243302321633479\n",
      "train loss:1.7045583965787687\n",
      "train loss:1.7773192569486118\n",
      "=== epoch:213, train acc:0.3233333333333333, test acc:0.2733 ===\n",
      "train loss:1.7591703382431072\n",
      "train loss:1.8051097448233044\n",
      "train loss:1.8296768814239994\n",
      "=== epoch:214, train acc:0.3566666666666667, test acc:0.2794 ===\n",
      "train loss:1.744226542663776\n",
      "train loss:1.7802447921052027\n",
      "train loss:1.7326612075099517\n",
      "=== epoch:215, train acc:0.3466666666666667, test acc:0.2821 ===\n",
      "train loss:1.7789255822689791\n",
      "train loss:1.7036961650765463\n",
      "train loss:1.6884118063460678\n",
      "=== epoch:216, train acc:0.35, test acc:0.2828 ===\n",
      "train loss:1.6918364336529863\n",
      "train loss:1.7906343797115418\n",
      "train loss:1.7911502631944745\n",
      "=== epoch:217, train acc:0.35333333333333333, test acc:0.2855 ===\n",
      "train loss:1.768972998016083\n",
      "train loss:1.7521038940003206\n",
      "train loss:1.7002608045655785\n",
      "=== epoch:218, train acc:0.36, test acc:0.2876 ===\n",
      "train loss:1.7737024883529022\n",
      "train loss:1.8443799206180358\n",
      "train loss:1.8355770981161488\n",
      "=== epoch:219, train acc:0.37666666666666665, test acc:0.2984 ===\n",
      "train loss:1.6245572029369122\n",
      "train loss:1.7534610690785284\n",
      "train loss:1.7102358039005328\n",
      "=== epoch:220, train acc:0.37, test acc:0.292 ===\n",
      "train loss:1.7153736033878348\n",
      "train loss:1.7260075658680194\n",
      "train loss:1.6050069840003658\n",
      "=== epoch:221, train acc:0.36666666666666664, test acc:0.2914 ===\n",
      "train loss:1.7487778866908235\n",
      "train loss:1.6247042134805774\n",
      "train loss:1.751373032892076\n",
      "=== epoch:222, train acc:0.37, test acc:0.2926 ===\n",
      "train loss:1.7374099655638786\n",
      "train loss:1.709620593639247\n",
      "train loss:1.67158460523906\n",
      "=== epoch:223, train acc:0.37666666666666665, test acc:0.2924 ===\n",
      "train loss:1.7214311114193277\n",
      "train loss:1.633917572036247\n",
      "train loss:1.6455359149136097\n",
      "=== epoch:224, train acc:0.38, test acc:0.2927 ===\n",
      "train loss:1.7184006589749676\n",
      "train loss:1.7044040532169553\n",
      "train loss:1.7088539162311394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:225, train acc:0.38333333333333336, test acc:0.2954 ===\n",
      "train loss:1.6678139755438541\n",
      "train loss:1.6862801558954694\n",
      "train loss:1.7161700682349872\n",
      "=== epoch:226, train acc:0.38666666666666666, test acc:0.301 ===\n",
      "train loss:1.7426245602296861\n",
      "train loss:1.6624641333921\n",
      "train loss:1.7098833811033263\n",
      "=== epoch:227, train acc:0.39, test acc:0.3028 ===\n",
      "train loss:1.6588521096021847\n",
      "train loss:1.663982666696046\n",
      "train loss:1.5602612052980798\n",
      "=== epoch:228, train acc:0.38333333333333336, test acc:0.3005 ===\n",
      "train loss:1.8423793641396822\n",
      "train loss:1.6602363068955213\n",
      "train loss:1.7408946644808805\n",
      "=== epoch:229, train acc:0.4, test acc:0.3064 ===\n",
      "train loss:1.6837434214926201\n",
      "train loss:1.6463411250212336\n",
      "train loss:1.6972973036559404\n",
      "=== epoch:230, train acc:0.4066666666666667, test acc:0.3055 ===\n",
      "train loss:1.7116561887807145\n",
      "train loss:1.592677916144438\n",
      "train loss:1.6647499149765883\n",
      "=== epoch:231, train acc:0.3933333333333333, test acc:0.3047 ===\n",
      "train loss:1.6622980269732721\n",
      "train loss:1.5885931268962508\n",
      "train loss:1.6697199180347662\n",
      "=== epoch:232, train acc:0.4166666666666667, test acc:0.3088 ===\n",
      "train loss:1.6737439650782802\n",
      "train loss:1.6603270483157855\n",
      "train loss:1.6396115596036913\n",
      "=== epoch:233, train acc:0.42333333333333334, test acc:0.3172 ===\n",
      "train loss:1.6183982118516465\n",
      "train loss:1.6399672015729192\n",
      "train loss:1.6499086744546312\n",
      "=== epoch:234, train acc:0.43, test acc:0.3162 ===\n",
      "train loss:1.7352560764686151\n",
      "train loss:1.636228103444018\n",
      "train loss:1.67726029839777\n",
      "=== epoch:235, train acc:0.43, test acc:0.3265 ===\n",
      "train loss:1.6420276279479864\n",
      "train loss:1.6420286561483008\n",
      "train loss:1.5208310950511108\n",
      "=== epoch:236, train acc:0.43, test acc:0.3255 ===\n",
      "train loss:1.6400728390779986\n",
      "train loss:1.5516616204587248\n",
      "train loss:1.7146259462860776\n",
      "=== epoch:237, train acc:0.45, test acc:0.3334 ===\n",
      "train loss:1.6907097809891116\n",
      "train loss:1.7228258222311608\n",
      "train loss:1.6348472921464685\n",
      "=== epoch:238, train acc:0.44666666666666666, test acc:0.3357 ===\n",
      "train loss:1.6388823705336266\n",
      "train loss:1.5610767050032073\n",
      "train loss:1.6381479130064072\n",
      "=== epoch:239, train acc:0.44666666666666666, test acc:0.3347 ===\n",
      "train loss:1.5893194128201582\n",
      "train loss:1.696810844812304\n",
      "train loss:1.6660179933755557\n",
      "=== epoch:240, train acc:0.43666666666666665, test acc:0.3362 ===\n",
      "train loss:1.6233849458714076\n",
      "train loss:1.6960578699878652\n",
      "train loss:1.7647318090805941\n",
      "=== epoch:241, train acc:0.45, test acc:0.3462 ===\n",
      "train loss:1.5342664329862798\n",
      "train loss:1.5042670324993819\n",
      "train loss:1.6155652149331474\n",
      "=== epoch:242, train acc:0.46, test acc:0.3495 ===\n",
      "train loss:1.600144465677744\n",
      "train loss:1.6611185085173736\n",
      "train loss:1.5495225491183562\n",
      "=== epoch:243, train acc:0.4533333333333333, test acc:0.3526 ===\n",
      "train loss:1.5885972823630865\n",
      "train loss:1.548814050758787\n",
      "train loss:1.5633711129508798\n",
      "=== epoch:244, train acc:0.45666666666666667, test acc:0.3549 ===\n",
      "train loss:1.556616981698208\n",
      "train loss:1.5979696948275288\n",
      "train loss:1.5819651894786566\n",
      "=== epoch:245, train acc:0.45666666666666667, test acc:0.3542 ===\n",
      "train loss:1.6318917702124391\n",
      "train loss:1.521960312455437\n",
      "train loss:1.6227487148117101\n",
      "=== epoch:246, train acc:0.4666666666666667, test acc:0.358 ===\n",
      "train loss:1.6451679293175232\n",
      "train loss:1.6490810804780016\n",
      "train loss:1.5662290582031477\n",
      "=== epoch:247, train acc:0.4666666666666667, test acc:0.3641 ===\n",
      "train loss:1.5591905200299803\n",
      "train loss:1.6317549902280772\n",
      "train loss:1.5395544509872252\n",
      "=== epoch:248, train acc:0.47333333333333333, test acc:0.368 ===\n",
      "train loss:1.6637892588772343\n",
      "train loss:1.5289902492864267\n",
      "train loss:1.5195283304560485\n",
      "=== epoch:249, train acc:0.48333333333333334, test acc:0.3712 ===\n",
      "train loss:1.6091189796103926\n",
      "train loss:1.5588300247994613\n",
      "train loss:1.6532593042499357\n",
      "=== epoch:250, train acc:0.48, test acc:0.3718 ===\n",
      "train loss:1.4782780958486856\n",
      "train loss:1.604749160851035\n",
      "train loss:1.539287061953929\n",
      "=== epoch:251, train acc:0.4866666666666667, test acc:0.3781 ===\n",
      "train loss:1.5413632116825338\n",
      "train loss:1.5257792004193593\n",
      "train loss:1.493554673722632\n",
      "=== epoch:252, train acc:0.49333333333333335, test acc:0.38 ===\n",
      "train loss:1.6375192108313084\n",
      "train loss:1.4834267169477906\n",
      "train loss:1.6687304530028804\n",
      "=== epoch:253, train acc:0.49666666666666665, test acc:0.3843 ===\n",
      "train loss:1.532082565495423\n",
      "train loss:1.5126186035547367\n",
      "train loss:1.484169516705924\n",
      "=== epoch:254, train acc:0.5033333333333333, test acc:0.3917 ===\n",
      "train loss:1.4862090784275777\n",
      "train loss:1.5496563373015388\n",
      "train loss:1.5875624857273596\n",
      "=== epoch:255, train acc:0.5133333333333333, test acc:0.3953 ===\n",
      "train loss:1.4633798303570793\n",
      "train loss:1.5209631823984568\n",
      "train loss:1.5833126736499796\n",
      "=== epoch:256, train acc:0.51, test acc:0.3946 ===\n",
      "train loss:1.6110796899838886\n",
      "train loss:1.5454817239818062\n",
      "train loss:1.487900253903347\n",
      "=== epoch:257, train acc:0.5066666666666667, test acc:0.3949 ===\n",
      "train loss:1.554408792659767\n",
      "train loss:1.456334751597162\n",
      "train loss:1.5203997946795977\n",
      "=== epoch:258, train acc:0.5, test acc:0.3915 ===\n",
      "train loss:1.5676420795446142\n",
      "train loss:1.509798522922704\n",
      "train loss:1.4899213483341618\n",
      "=== epoch:259, train acc:0.5133333333333333, test acc:0.3945 ===\n",
      "train loss:1.5236198996152563\n",
      "train loss:1.5329980482050445\n",
      "train loss:1.518450232151575\n",
      "=== epoch:260, train acc:0.5, test acc:0.3946 ===\n",
      "train loss:1.5216230224389151\n",
      "train loss:1.5883901458690277\n",
      "train loss:1.584709880767938\n",
      "=== epoch:261, train acc:0.5233333333333333, test acc:0.4046 ===\n",
      "train loss:1.5769573591048789\n",
      "train loss:1.4815122912538916\n",
      "train loss:1.5570943689035324\n",
      "=== epoch:262, train acc:0.5266666666666666, test acc:0.4058 ===\n",
      "train loss:1.5064854402576384\n",
      "train loss:1.4326776201256854\n",
      "train loss:1.5077058038532578\n",
      "=== epoch:263, train acc:0.5233333333333333, test acc:0.4083 ===\n",
      "train loss:1.4208993378386376\n",
      "train loss:1.4918425449916919\n",
      "train loss:1.5067182939887633\n",
      "=== epoch:264, train acc:0.54, test acc:0.4175 ===\n",
      "train loss:1.46671811416409\n",
      "train loss:1.4098971168487475\n",
      "train loss:1.5172905323444068\n",
      "=== epoch:265, train acc:0.5466666666666666, test acc:0.4199 ===\n",
      "train loss:1.418364043504121\n",
      "train loss:1.404792951321607\n",
      "train loss:1.546741812699547\n",
      "=== epoch:266, train acc:0.5566666666666666, test acc:0.4261 ===\n",
      "train loss:1.4535956978547357\n",
      "train loss:1.4039496761512533\n",
      "train loss:1.478981237582774\n",
      "=== epoch:267, train acc:0.56, test acc:0.43 ===\n",
      "train loss:1.410995634962272\n",
      "train loss:1.376743013848226\n",
      "train loss:1.3756354492115026\n",
      "=== epoch:268, train acc:0.5633333333333334, test acc:0.4324 ===\n",
      "train loss:1.3889526182272887\n",
      "train loss:1.482853111317293\n",
      "train loss:1.4040516892086472\n",
      "=== epoch:269, train acc:0.5666666666666667, test acc:0.4365 ===\n",
      "train loss:1.5304194067318158\n",
      "train loss:1.493103463066105\n",
      "train loss:1.4981232802332034\n",
      "=== epoch:270, train acc:0.5633333333333334, test acc:0.4405 ===\n",
      "train loss:1.482325090344972\n",
      "train loss:1.465106778651986\n",
      "train loss:1.5110210887247189\n",
      "=== epoch:271, train acc:0.5766666666666667, test acc:0.4406 ===\n",
      "train loss:1.4280556546789693\n",
      "train loss:1.450815214905647\n",
      "train loss:1.3010798037042541\n",
      "=== epoch:272, train acc:0.57, test acc:0.4372 ===\n",
      "train loss:1.423947957467964\n",
      "train loss:1.4120073296542048\n",
      "train loss:1.3768368171978582\n",
      "=== epoch:273, train acc:0.5833333333333334, test acc:0.4498 ===\n",
      "train loss:1.4699123898250304\n",
      "train loss:1.3468699265764934\n",
      "train loss:1.453583122952515\n",
      "=== epoch:274, train acc:0.5833333333333334, test acc:0.4612 ===\n",
      "train loss:1.4246061402350767\n",
      "train loss:1.3200019453393796\n",
      "train loss:1.4218639304136909\n",
      "=== epoch:275, train acc:0.5833333333333334, test acc:0.4623 ===\n",
      "train loss:1.3757554094253535\n",
      "train loss:1.3080230102298784\n",
      "train loss:1.3298945023953679\n",
      "=== epoch:276, train acc:0.5866666666666667, test acc:0.4663 ===\n",
      "train loss:1.271121582427008\n",
      "train loss:1.298295483580608\n",
      "train loss:1.377801917444707\n",
      "=== epoch:277, train acc:0.5766666666666667, test acc:0.4657 ===\n",
      "train loss:1.4420955703882727\n",
      "train loss:1.336853817542605\n",
      "train loss:1.3398954050758065\n",
      "=== epoch:278, train acc:0.5866666666666667, test acc:0.4706 ===\n",
      "train loss:1.3181686315858316\n",
      "train loss:1.4172542524153209\n",
      "train loss:1.380297035400026\n",
      "=== epoch:279, train acc:0.5966666666666667, test acc:0.4709 ===\n",
      "train loss:1.3117071808675562\n",
      "train loss:1.395082215658473\n",
      "train loss:1.1957445326480263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:280, train acc:0.5866666666666667, test acc:0.4597 ===\n",
      "train loss:1.2994930278845873\n",
      "train loss:1.2334711505392424\n",
      "train loss:1.3399124367777646\n",
      "=== epoch:281, train acc:0.5866666666666667, test acc:0.4602 ===\n",
      "train loss:1.3894967045833886\n",
      "train loss:1.2970424374770682\n",
      "train loss:1.3423501445705823\n",
      "=== epoch:282, train acc:0.59, test acc:0.4666 ===\n",
      "train loss:1.3423596091621233\n",
      "train loss:1.2558375123406385\n",
      "train loss:1.4014618735251199\n",
      "=== epoch:283, train acc:0.5966666666666667, test acc:0.4694 ===\n",
      "train loss:1.4293745842220356\n",
      "train loss:1.2896294494150744\n",
      "train loss:1.3297467565034466\n",
      "=== epoch:284, train acc:0.5966666666666667, test acc:0.473 ===\n",
      "train loss:1.3441167195437165\n",
      "train loss:1.3048437344091597\n",
      "train loss:1.2936578953023652\n",
      "=== epoch:285, train acc:0.6, test acc:0.4755 ===\n",
      "train loss:1.1849023982320168\n",
      "train loss:1.4324398022938984\n",
      "train loss:1.3243639112567425\n",
      "=== epoch:286, train acc:0.6066666666666667, test acc:0.4757 ===\n",
      "train loss:1.469519420240936\n",
      "train loss:1.273081439819973\n",
      "train loss:1.3502605488317143\n",
      "=== epoch:287, train acc:0.6033333333333334, test acc:0.4762 ===\n",
      "train loss:1.330045590570406\n",
      "train loss:1.2081944028072025\n",
      "train loss:1.317753690041348\n",
      "=== epoch:288, train acc:0.6033333333333334, test acc:0.4784 ===\n",
      "train loss:1.2850494396650773\n",
      "train loss:1.2846944777723457\n",
      "train loss:1.2794715002155106\n",
      "=== epoch:289, train acc:0.6, test acc:0.4808 ===\n",
      "train loss:1.1996552746490436\n",
      "train loss:1.3518372552589955\n",
      "train loss:1.2268080470912062\n",
      "=== epoch:290, train acc:0.6066666666666667, test acc:0.4864 ===\n",
      "train loss:1.3659022046098246\n",
      "train loss:1.3619086695061677\n",
      "train loss:1.4170012849128744\n",
      "=== epoch:291, train acc:0.6033333333333334, test acc:0.4895 ===\n",
      "train loss:1.2119935305788394\n",
      "train loss:1.2254748991879003\n",
      "train loss:1.301248721855109\n",
      "=== epoch:292, train acc:0.6266666666666667, test acc:0.4948 ===\n",
      "train loss:1.4042731029245092\n",
      "train loss:1.1356644853651374\n",
      "train loss:1.3749067728870963\n",
      "=== epoch:293, train acc:0.6066666666666667, test acc:0.4878 ===\n",
      "train loss:1.2679441553111528\n",
      "train loss:1.2129480330486853\n",
      "train loss:1.254573875625325\n",
      "=== epoch:294, train acc:0.5966666666666667, test acc:0.4846 ===\n",
      "train loss:1.3195969728302006\n",
      "train loss:1.1090135172927917\n",
      "train loss:1.1944046453099293\n",
      "=== epoch:295, train acc:0.6166666666666667, test acc:0.4929 ===\n",
      "train loss:1.319253622328474\n",
      "train loss:1.3088749747411927\n",
      "train loss:1.1714417188445587\n",
      "=== epoch:296, train acc:0.6266666666666667, test acc:0.5008 ===\n",
      "train loss:1.2943820478820192\n",
      "train loss:1.1842327731057427\n",
      "train loss:1.4205427179536878\n",
      "=== epoch:297, train acc:0.63, test acc:0.5057 ===\n",
      "train loss:1.2626020672240899\n",
      "train loss:1.2150570091094681\n",
      "train loss:1.3107434601353134\n",
      "=== epoch:298, train acc:0.6366666666666667, test acc:0.5116 ===\n",
      "train loss:1.2275544709925044\n",
      "train loss:1.238627780188863\n",
      "train loss:1.1371306517291537\n",
      "=== epoch:299, train acc:0.6333333333333333, test acc:0.5183 ===\n",
      "train loss:1.0481590911712662\n",
      "train loss:1.3147168227384938\n",
      "train loss:1.2548804392273034\n",
      "=== epoch:300, train acc:0.6333333333333333, test acc:0.531 ===\n",
      "train loss:1.1375195682679204\n",
      "train loss:1.2124256948612953\n",
      "train loss:1.2575101551263708\n",
      "=== epoch:301, train acc:0.63, test acc:0.5231 ===\n",
      "train loss:1.1633592028066495\n",
      "train loss:1.2846897055847273\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5235\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23187d70cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer # 신경망 훈련을 대신해주는 클래스\n",
    "\n",
    "'''\n",
    "드롭아웃(Dropout) 구현. (common.layers에 class로 구현되어있음)\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    ---훈련시에는 순전파때마다 self.mask에 삭제할 뉴런을 False로 표시한다.---\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            --- self.mask에 x와 형상이 같은 배열을 무작위로 생성하고 dropout_ration보다 큰 원소만 True로 설정한다. ---\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x*self.mask\n",
    "        else:\n",
    "            return x*(1.0-self.mask)\n",
    "\n",
    "    --- 역전파는 ReLU와 같은 동작을 한다. 순전파때 신호를 통과시킨 뉴런은 역전파때도 그대로 통과시키고,\n",
    "        그렇지 않은 뉴런은 역전파때 신호를 차단한다. ---\n",
    "    def backward(self, dout):\n",
    "        return dout*self.mask\n",
    "'''\n",
    "\n",
    "# 데이터 불러오기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 훈련데이터를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "# 신경망 학습\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                            use_dropout=True, dropout_ration = dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=301, mini_batch_size=100,\n",
    "                    optimizer='sgd', optimizer_param={'lr':0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
